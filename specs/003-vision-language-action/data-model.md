# Conceptual Data Model: Vision-Language-Action (VLA) Module

This document outlines the key conceptual entities for the Vision-Language-Action (VLA) pipeline.

- **Entity: Voice Command**
  - **Description**: The raw audio input from a user.
  - **Attributes**: `audio_data`, `timestamp`

- **Entity: Text Transcript**
  - **Description**: The textual representation of a user's voice command.
  - **Attributes**: `text`, `confidence_score`
  - **Relationships**: Generated from a `Voice Command` by a Speech-to-Text model.

- **Entity: Task Plan**
  - **Description**: A sequence of high-level, discrete tasks generated by an LLM to fulfill the user's intent.
  - **Attributes**: `task_list` (e.g., `["find red ball", "pick up red ball", "bring to user"]`)
  - **Relationships**: Generated from a `Text Transcript`.

- **Entity: Action Sequence**
  - **Description**: A sequence of low-level, executable ROS 2 actions translated from the `Task Plan`.
  - **Attributes**: `action_list` (e.g., `[nav2_goal(x,y), manipulation_goal(grasp), nav2_goal(x,y)]`)
  - **Relationships**: Generated from a `Task Plan`.

- **Entity: Vision Data**
  - **Description**: Real-time visual input from the robot's cameras.
  - **Attributes**: `image_stream`, `depth_data`, `object_detections`
  - **Relationships**: Consumed by the perception and navigation systems.

- **Entity: World State**
  - **Description**: The robot's internal, real-time understanding of its environment.
  - **Attributes**: `robot_pose`, `map`, `object_locations`
  - **Relationships**: Continuously updated by `Vision Data` and other sensors via VSLAM and other perception modules.

- **Entity: Robot Action**
  - **Description**: A single, executable action for the robot.
  - **Types**: `Navigation`, `Manipulation`, `Speech`
  - **Relationships**: Executed by the robot's controllers via ROS 2.
